# 💫 Web Scraping Journey: 30-Day Growth Plan
From Static Data to Dynamic Rendering | Unlock New Skills Daily, Build a Comprehensive Knowledge System

## 🎯 Project Overview

Yo, you guys have all heard of or played side-scroller games, right? You know, grinding through levels, smashing countless monsters, failing a bazillion times, but then finally beating that OP final boss? That feeling is legit amazing, right? o(￣▽￣)o

To make learning web crawlers way more fun, this time we're gonna level up and crush monsters too! We'll tackle challenging stages one by one, starting easy and going deep, exploring the whole world of web crawlers. LFG! (Let's gooo!)

This repository documents my systematic learning journey in web scraping technology. <br>

Through 30 carefully designed tasks, I will progressively master:

🏆 End-to-end skills from basic web parsing to dynamic rendering

🛡️ Practical solutions for various anti-scraping mechanisms

📊 Professional methods for data cleaning, storage, and visualization

🧩 Advanced applications in reverse engineering and distributed crawling

Each task is designed for real websites, ensuring learned skills are directly applicable to actual projects.


## 📂 Repository Structure
 ```
.
├── 📁 0_Bootcamp/          # Basic Training (Days 1-7)
│   └── Day1_HTML_Parsing/
├── 📁 1_Static_Quests/     # Static Scraping Advanced (Days 8-14)
│   └── Day8_Header_Disguise/
├── 📁 2_Dynamic_Dungeons/   # Dynamic Page Handling (Days 15-21)
│   └── Day15_Selenium_Basics/
├── 📁 3_Boss_Battles/     # Advanced Techniques (Days 22-30)
│   └── Day22_JS_Reverse_Engineering/
├── 📜 Adventure_Log.md  # Learning Progress Tracker
├── 📜 Survival_Guide.md     # Technical Reference Guide
└── 📜 README_EN.md            # Project Documentation
 ```

## 🚀 Daily Task System
Each task is a complete learning unit:

 ```
Day1_HTML_Parsing/
├── task.md            # Task description & notes
├── scraper.py         # Main scraper code
├── requirements.txt   # Dependencies
├── output/            # Output files
└── knowledge_base.md  # Extended knowledge
 ```

## ⚙️ Technical System
Core Tech Stack:

* Data Acquisition: Requests, HTTPX

* Page Parsing: BeautifulSoup, lxml, PyQuery

* Dynamic Rendering: Selenium, Playwright

* Data Management: Pandas, SQLAlchemy

* Advanced Applications: Scrapy, Mitmproxy

## 🌟 Join the Journey
```
# Clone repository
git clone https://github.com/yourname/web-scraping-journey.git

# Navigate to Day1 task
cd 0_Bootcamp/Day1_HTML_Parsing

# Install dependencies
pip install -r requirements.txt

# Run scraper
python scraper.py
```
### Learning Tips
* Daily Focus: Complete one core task daily

* Knowledge Organization: Record key points in task.md

* Code Refinement: Review and improve previous implementations weekly

* Practical Application: Apply techniques to real-world scenarios

# 📚 Learning Resources
* [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
* [Scrapy Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)
* To be continued...

Maintainer: Jade Cao
Last Updated: 13/08/2025

"Code is the best study note, commit history is the witness of growth.
Progress a little each day, and you'll be amazed at your transformation after 30 days."
— Encouragement to all learners

Thank you for your reading! All feedback is welcome. o(=•ェ•=)m

